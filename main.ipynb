{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50.007 Machine Learning, Spring 2025\n",
    "# Design Project\n",
    "\n",
    "Due 27 Apr 2025, 5:00pm\n",
    "\n",
    "By: Aishwarya Iyer (1007141) and Khoo Zi Qi (1006984)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 (30points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "train_file_path = \"EN/train\"  # Adjust if necessary\n",
    "print(\"File exists:\", os.path.exists(train_file_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that estimates the emission parameters from the training set using MLE (maximum likelihood estimation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Computes emission parameters for an HMM: e(x|y) = Count(y → x) / Count(y)\n",
    "where:\n",
    "- x: observed word\n",
    "- y: corresponding tag (e.g., 'B-NP', 'I-VP', 'O')\n",
    "\"\"\"\n",
    "# Use defaultdict to automatically handles missing keys\n",
    "from collections import defaultdict\n",
    "\n",
    "def compute_emission_parameters(train_file_path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        train_file_path: Path to training file (word-tag pairs separated by whitespace)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of dictionaries: emission_parameters[tag][word] = probability\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize counters:\n",
    "    # - emission_counts[tag][word] = times word appears with tag\n",
    "    # - tag_counts[tag] = total occurrences of tag\n",
    "    emission_counts = defaultdict(lambda: defaultdict(int))\n",
    "    tag_counts = defaultdict(int)\n",
    "\n",
    "    # Count word-tag co-occurrences and tag frequencies\n",
    "    with open(train_file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:  # Skip empty lines\n",
    "                try:\n",
    "                    word, tag = line.split()  # Split by any whitespace\n",
    "                    emission_counts[tag][word] += 1\n",
    "                    tag_counts[tag] += 1\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping invalid line: {line}\")\n",
    "\n",
    "    # Calculate emission probabilities\n",
    "    emission_parameters = defaultdict(dict)\n",
    "    for tag in emission_counts:\n",
    "        total_tag_occurrences = tag_counts[tag]\n",
    "        for word in emission_counts[tag]:\n",
    "            emission_parameters[tag][word] = (\n",
    "                emission_counts[tag][word] / total_tag_occurrences\n",
    "            )\n",
    "    \n",
    "    return emission_parameters\n",
    "\n",
    "emission_parameters = compute_emission_parameters(train_file_path)\n",
    "# print(emission_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use smoothing\n",
    "- Identify words that appear less than 3 times\n",
    "- Replace those words with #UNK#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_emission_parameters_smoothing(train_file_path, k):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        train_file_path: Path to training file (word-tag pairs separated by whitespace)\n",
    "        k: minimum count of word. If word count less than k, replace word with #UNK#.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of dictionaries: emission_parameters[tag][word] = probability\n",
    "    \"\"\"\n",
    "    \n",
    "    word_counts = defaultdict(int)\n",
    "    with open(train_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:  # not an empty line\n",
    "                word, tag = line.split()\n",
    "                word_counts[word] += 1\n",
    "    \n",
    "    # Identify rare words\n",
    "    rare_words = {word for word, count in word_counts.items() if count < k}\n",
    "\n",
    "    # Initialize counters:\n",
    "    # - emission_counts[tag][word] = times word appears with tag\n",
    "    # - tag_counts[tag] = total occurrences of tag\n",
    "    emission_counts = defaultdict(lambda: defaultdict(int))\n",
    "    tag_counts = defaultdict(int)\n",
    "\n",
    "    # Count word-tag co-occurrences and tag frequencies\n",
    "    with open(train_file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:  # Skip empty lines\n",
    "                try:\n",
    "                    word, tag = line.split()  # Split by any whitespace\n",
    "                    processed_word = word if word not in rare_words else '#UNK#' #modify training set\n",
    "                    emission_counts[tag][processed_word] += 1\n",
    "                    tag_counts[tag] += 1\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping invalid line: {line}\")\n",
    "\n",
    "    # Calculate emission probabilities\n",
    "    emission_parameters = defaultdict(dict)\n",
    "    for tag in emission_counts:\n",
    "        total_tag_occurrences = tag_counts[tag]\n",
    "        for word in emission_counts[tag]:\n",
    "            emission_parameters[tag][word] = (\n",
    "                emission_counts[tag][word] / total_tag_occurrences\n",
    "            )\n",
    "    \n",
    "    return emission_parameters\n",
    "\n",
    "emission_parameters_smoothing = compute_emission_parameters_smoothing(train_file_path, k = 3)\n",
    "# print(emission_parameters_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a simple system that produces the tag\n",
    "y∗= arg maxy e(x|y)\n",
    "for each word x in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_in_file_path = 'EN/dev.in'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tags(dev_in_file_path, emission_parameters, unknown_tag='O'):\n",
    "    \"\"\"\n",
    "    Predicts tags for a sentence using emission probabilities.\n",
    "    \n",
    "    Args:\n",
    "        sentence: List of words to tag\n",
    "        emission_params: Dictionary from compute_emission_parameters()\n",
    "        unknown_tag: Default tag for unseen words\n",
    "    \n",
    "    Returns:\n",
    "        List of (word, predicted_tag) tuples\n",
    "    \"\"\"\n",
    "    predicted = []\n",
    "    \n",
    "    with open(dev_in_file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            word = line.strip()\n",
    "            if word:  # Skip empty lines, each line has one word\n",
    "                max_prob = -1\n",
    "                best_tag = unknown_tag  # Default fallback\n",
    "                try:\n",
    "                    # Find tag with highest emission probability for this word\n",
    "                    for tag in emission_parameters:\n",
    "                        if word in emission_parameters[tag]:\n",
    "                            if emission_parameters[tag][word] > max_prob:\n",
    "                                max_prob = emission_parameters[tag][word]\n",
    "                                best_tag = tag\n",
    "                    \n",
    "                    predicted.append((word, best_tag))\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping invalid line: {line}\")         \n",
    "    return predicted\n",
    "\n",
    "predicted_list = predict_tags(dev_in_file_path, emission_parameters_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn these parameters with train, and evaluate your system on the development set dev.in for\n",
    "each of the dataset. Write your output to dev.p2.out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions(predicted_list, output_file_path):\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as fout:\n",
    "        for word, tag in predicted_list:\n",
    "            fout.write(f\"{word} {tag}\\n\")\n",
    "\n",
    "output_file_path = 'outputs/dev.p2.out'\n",
    "write_predictions(predicted_list, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your outputs and the gold-standard outputs in dev.out and report the precision, recall and F scores of such a baseline system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chunks(tag_sequence):\n",
    "    \"\"\"Convert tag sequence to list of (start_idx, end_idx, chunk_type) tuples\"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = None\n",
    "    \n",
    "    for i, tag in enumerate(tag_sequence):\n",
    "        if tag.startswith('B-'):\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "            current_chunk = (i, i+1, tag[2:])\n",
    "        elif tag.startswith('I-'):\n",
    "            if current_chunk and current_chunk[2] == tag[2:]:\n",
    "                current_chunk = (current_chunk[0], i+1, current_chunk[2])\n",
    "            else:\n",
    "                # Invalid transition (O → I), treat as B-\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk)\n",
    "                current_chunk = (i, i+1, tag[2:])\n",
    "        else:  # O\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "            current_chunk = None\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6478\n",
      "Recall:    0.6575\n",
      "F1 Score:  0.6526\n",
      "True Positives:  526\n",
      "False Positives: 286\n",
      "False Negatives: 274\n"
     ]
    }
   ],
   "source": [
    "def evaluate(gold_file, pred_file):\n",
    "    \"\"\"Calculate precision, recall and F1\"\"\"\n",
    "    gold_chunks = []\n",
    "    pred_chunks = []\n",
    "    \n",
    "    # Read both files simultaneously\n",
    "    with open(gold_file, 'r', encoding='utf-8') as fgold, \\\n",
    "         open(pred_file, 'r', encoding='utf-8') as fpred:\n",
    "        \n",
    "        gold_sentence = []\n",
    "        pred_sentence = []\n",
    "        \n",
    "        for gold_line, pred_line in zip(fgold, fpred):\n",
    "            gold_line = gold_line.strip()\n",
    "            pred_line = pred_line.strip()\n",
    "            \n",
    "            if gold_line and pred_line:\n",
    "                # Get tags (assuming format: word\\tTag)\n",
    "                gold_tag = gold_line.split()[1]\n",
    "                pred_tag = pred_line.split()[1]\n",
    "                gold_sentence.append(gold_tag)\n",
    "                pred_sentence.append(pred_tag)\n",
    "            else:\n",
    "                # End of sentence\n",
    "                if gold_sentence and pred_sentence:\n",
    "                    gold_chunks.extend(extract_chunks(gold_sentence))\n",
    "                    pred_chunks.extend(extract_chunks(pred_sentence))\n",
    "                gold_sentence = []\n",
    "                pred_sentence = []\n",
    "    \n",
    "    # Calculate metrics\n",
    "    gold_set = set(gold_chunks)\n",
    "    pred_set = set(pred_chunks)\n",
    "    \n",
    "    tp = len(gold_set & pred_set)  # True positives\n",
    "    fp = len(pred_set - gold_set)  # False positives\n",
    "    fn = len(gold_set - pred_set)  # False negatives\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn\n",
    "    }\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    \"\"\"Pretty-print evaluation metrics\"\"\"\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score:  {metrics['f1']:.4f}\")\n",
    "    print(f\"True Positives:  {metrics['tp']}\")\n",
    "    print(f\"False Positives: {metrics['fp']}\")\n",
    "    print(f\"False Negatives: {metrics['fn']}\")\n",
    "\n",
    "metrics = evaluate('EN/dev.out', 'outputs/dev.p2.out')\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transition_parameters(train_file_path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        train_file_path: Path to training file (word-tag pairs separated by whitespace)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of dictionaries: transition_parameters[prev_tag][tag] = probability\n",
    "    \"\"\"\n",
    "    # Initialize counters:\n",
    "    # - transition_counts[prev_tag][tag] = times prev_tag transitions to tag\n",
    "    # - prev_tag_counts[prev_tag] = total occurrences of prev_tag\n",
    "    transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "    prev_tag_counts = defaultdict(int)\n",
    "\n",
    "    # Count tag transitions and previous tag frequencies\n",
    "    with open(train_file_path, 'r', encoding='utf-8') as file:\n",
    "        prev_tag = 'START'\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:  # Skip empty lines\n",
    "                try:\n",
    "                    _, tag = line.split()  # Split by any whitespace\n",
    "                    transition_counts[prev_tag][tag] += 1\n",
    "                    prev_tag_counts[prev_tag] += 1\n",
    "                    prev_tag = tag\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping invalid line: {line}\")\n",
    "\n",
    "    # Calculate transition probabilities\n",
    "    transition_parameters = defaultdict(dict)\n",
    "    total_prev_tag_occurrences = sum(prev_tag_counts.values())\n",
    "    for prev_tag in transition_counts:\n",
    "        total_tag_occurrences = prev_tag_counts[prev_tag]\n",
    "        for tag in transition_counts[prev_tag]:\n",
    "            transition_parameters[prev_tag][tag] = (\n",
    "                transition_counts[prev_tag][tag] / total_tag_occurrences\n",
    "            )\n",
    "    \n",
    "    # Add special cases for q(STOP|yn) and q(y1|START)\n",
    "    transition_parameters['STOP'] = defaultdict(int)\n",
    "    transition_parameters['START'] = defaultdict(int)\n",
    "    transition_parameters['START']['y1'] = 1.0\n",
    "    \n",
    "    return transition_parameters\n",
    "\n",
    "transition_parameters = compute_transition_parameters(train_file_path)\n",
    "#print(transition_parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
