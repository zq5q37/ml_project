{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50.007 Machine Learning, Spring 2025\n",
    "# Design Project\n",
    "\n",
    "Due 27 Apr 2025, 5:00pm\n",
    "\n",
    "By: Aishwarya Iyer (1007141) and Khoo Zi Qi (1006984)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 (30points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: True\n"
     ]
    }
   ],
   "source": [
    "train_file_path = \"EN/train\"  # Adjust if necessary\n",
    "print(\"File exists:\", os.path.exists(train_file_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that estimates the emission parameters from the training set using MLE (maximum likelihood estimation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Computes emission parameters for an HMM: e(x|y) = Count(y → x) / Count(y)\n",
    "where:\n",
    "- x: observed word\n",
    "- y: corresponding tag (e.g., 'B-NP', 'I-VP', 'O')\n",
    "\"\"\"\n",
    "# Use defaultdict to automatically handles missing keys\n",
    "from collections import defaultdict\n",
    "\n",
    "def compute_emission_parameters(train_file_path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        train_file_path: Path to training file (word-tag pairs separated by whitespace)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of dictionaries: emission_parameters[tag][word] = probability\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize counters:\n",
    "    # - emission_counts[tag][word] = times word appears with tag\n",
    "    # - tag_counts[tag] = total occurrences of tag\n",
    "    emission_counts = defaultdict(lambda: defaultdict(int))\n",
    "    tag_counts = defaultdict(int)\n",
    "\n",
    "    # Count word-tag co-occurrences and tag frequencies\n",
    "    with open(train_file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:  # Skip empty lines\n",
    "                try:\n",
    "                    word, tag = line.split()  # Split by any whitespace\n",
    "                    emission_counts[tag][word] += 1\n",
    "                    tag_counts[tag] += 1\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping invalid line: {line}\")\n",
    "\n",
    "    # Calculate emission probabilities\n",
    "    emission_parameters = defaultdict(dict)\n",
    "    for tag in emission_counts:\n",
    "        total_tag_occurrences = tag_counts[tag]\n",
    "        for word in emission_counts[tag]:\n",
    "            emission_parameters[tag][word] = (\n",
    "                emission_counts[tag][word] / total_tag_occurrences\n",
    "            )\n",
    "    \n",
    "    return emission_parameters\n",
    "\n",
    "emission_parameters = compute_emission_parameters(train_file_path)\n",
    "# print(emission_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use smoothing\n",
    "- Identify words that appear less than 3 times\n",
    "- Replace those words with #UNK#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_emission_parameters_smoothing(train_file_path, k):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        train_file_path: Path to training file (word-tag pairs separated by whitespace)\n",
    "        k: minimum count of word. If word count less than k, replace word with #UNK#.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of dictionaries: emission_parameters[tag][word] = probability\n",
    "    \"\"\"\n",
    "    \n",
    "    word_counts = defaultdict(int)\n",
    "    with open(train_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:  # not an empty line\n",
    "                word, tag = line.split()\n",
    "                word_counts[word] += 1\n",
    "    \n",
    "    # Identify rare words\n",
    "    rare_words = {word for word, count in word_counts.items() if count < k}\n",
    "\n",
    "    # Initialize counters:\n",
    "    # - emission_counts[tag][word] = times word appears with tag\n",
    "    # - tag_counts[tag] = total occurrences of tag\n",
    "    emission_counts = defaultdict(lambda: defaultdict(int))\n",
    "    tag_counts = defaultdict(int)\n",
    "\n",
    "    # Count word-tag co-occurrences and tag frequencies\n",
    "    with open(train_file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:  # Skip empty lines\n",
    "                try:\n",
    "                    word, tag = line.split()  # Split by any whitespace\n",
    "                    processed_word = word if word not in rare_words else '#UNK#' #modify training set\n",
    "                    emission_counts[tag][processed_word] += 1\n",
    "                    tag_counts[tag] += 1\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping invalid line: {line}\")\n",
    "\n",
    "    # Calculate emission probabilities\n",
    "    emission_parameters = defaultdict(dict)\n",
    "    for tag in emission_counts:\n",
    "        total_tag_occurrences = tag_counts[tag]\n",
    "        for word in emission_counts[tag]:\n",
    "            emission_parameters[tag][word] = (\n",
    "                emission_counts[tag][word] / total_tag_occurrences\n",
    "            )\n",
    "    \n",
    "    return emission_parameters\n",
    "\n",
    "emission_parameters_smoothing = compute_emission_parameters_smoothing(train_file_path, k = 3)\n",
    "# print(emission_parameters_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a simple system that produces the tag\n",
    "y∗= arg maxy e(x|y)\n",
    "for each word x in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_in_file_path = 'EN/dev.in'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tags(dev_in_file_path, emission_parameters, unknown_tag='O'):\n",
    "    \"\"\"\n",
    "    Predicts tags for a sentence using emission probabilities.\n",
    "    \n",
    "    Args:\n",
    "        sentence: List of words to tag\n",
    "        emission_params: Dictionary from compute_emission_parameters()\n",
    "        unknown_tag: Default tag for unseen words\n",
    "    \n",
    "    Returns:\n",
    "        List of (word, predicted_tag) tuples\n",
    "    \"\"\"\n",
    "    predicted = []\n",
    "    \n",
    "    with open(dev_in_file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            word = line.strip()\n",
    "            if word:  # Skip empty lines, each line has one word\n",
    "                max_prob = -1\n",
    "                best_tag = unknown_tag  # Default fallback\n",
    "                try:\n",
    "                    # Find tag with highest emission probability for this word\n",
    "                    for tag in emission_parameters:\n",
    "                        if word in emission_parameters[tag]:\n",
    "                            if emission_parameters[tag][word] > max_prob:\n",
    "                                max_prob = emission_parameters[tag][word]\n",
    "                                best_tag = tag\n",
    "                    \n",
    "                    predicted.append((word, best_tag))\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping invalid line: {line}\")         \n",
    "    return predicted\n",
    "\n",
    "predicted_list = predict_tags(dev_in_file_path, emission_parameters_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn these parameters with train, and evaluate your system on the development set dev.in for\n",
    "each of the dataset. Write your output to dev.p2.out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions(predicted_list, output_file_path):\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as fout:\n",
    "        for word, tag in predicted_list:\n",
    "            fout.write(f\"{word} {tag}\\n\")\n",
    "\n",
    "output_file_path = 'outputs/dev.p2.out'\n",
    "write_predictions(predicted_list, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your outputs and the gold-standard outputs in dev.out and report the precision, recall and F scores of such a baseline system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chunks(tag_sequence):\n",
    "    \"\"\"Convert tag sequence to list of (start_idx, end_idx, chunk_type) tuples\"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = None\n",
    "    \n",
    "    for i, tag in enumerate(tag_sequence):\n",
    "        if tag.startswith('B-'):\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "            current_chunk = (i, i+1, tag[2:])\n",
    "        elif tag.startswith('I-'):\n",
    "            if current_chunk and current_chunk[2] == tag[2:]:\n",
    "                current_chunk = (current_chunk[0], i+1, current_chunk[2])\n",
    "            else:\n",
    "                # Invalid transition (O → I), treat as B-\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk)\n",
    "                current_chunk = (i, i+1, tag[2:])\n",
    "        else:  # O\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "            current_chunk = None\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6478\n",
      "Recall:    0.6575\n",
      "F1 Score:  0.6526\n",
      "True Positives:  526\n",
      "False Positives: 286\n",
      "False Negatives: 274\n"
     ]
    }
   ],
   "source": [
    "def evaluate(gold_file, pred_file):\n",
    "    \"\"\"Calculate precision, recall and F1\"\"\"\n",
    "    gold_chunks = []\n",
    "    pred_chunks = []\n",
    "    \n",
    "    # Read both files simultaneously\n",
    "    with open(gold_file, 'r', encoding='utf-8') as fgold, \\\n",
    "         open(pred_file, 'r', encoding='utf-8') as fpred:\n",
    "        \n",
    "        gold_sentence = []\n",
    "        pred_sentence = []\n",
    "        \n",
    "        for gold_line, pred_line in zip(fgold, fpred):\n",
    "            gold_line = gold_line.strip()\n",
    "            pred_line = pred_line.strip()\n",
    "            \n",
    "            if gold_line and pred_line:\n",
    "                # Get tags (assuming format: word\\tTag)\n",
    "                gold_tag = gold_line.split()[1]\n",
    "                pred_tag = pred_line.split()[1]\n",
    "                gold_sentence.append(gold_tag)\n",
    "                pred_sentence.append(pred_tag)\n",
    "            else:\n",
    "                # End of sentence\n",
    "                if gold_sentence and pred_sentence:\n",
    "                    gold_chunks.extend(extract_chunks(gold_sentence))\n",
    "                    pred_chunks.extend(extract_chunks(pred_sentence))\n",
    "                gold_sentence = []\n",
    "                pred_sentence = []\n",
    "    \n",
    "    # Calculate metrics\n",
    "    gold_set = set(gold_chunks)\n",
    "    pred_set = set(pred_chunks)\n",
    "    \n",
    "    tp = len(gold_set & pred_set)  # True positives\n",
    "    fp = len(pred_set - gold_set)  # False positives\n",
    "    fn = len(gold_set - pred_set)  # False negatives\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn\n",
    "    }\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    \"\"\"Pretty-print evaluation metrics\"\"\"\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score:  {metrics['f1']:.4f}\")\n",
    "    print(f\"True Positives:  {metrics['tp']}\")\n",
    "    print(f\"False Positives: {metrics['fp']}\")\n",
    "    print(f\"False Negatives: {metrics['fn']}\")\n",
    "\n",
    "metrics = evaluate('EN/dev.out', 'outputs/dev.p2.out')\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'dict'>, {'START': {'B-ADVP': 0.05428683283309409, 'I-ADVP': 0.0, 'I-NP': 0.0, 'B-PP': 0.1087041628604985, 'B-ADJP': 0.003262429857758058, 'I-INTJ': 0.0, 'B-LST': 0.0010439775544825785, 'I-UCP': 0.0, 'I-SBAR': 0.0, 'B-VP': 0.018661098786376094, 'I-CONJP': 0.0, 'B-SBAR': 0.02257601461568576, 'B-UCP': 0.0, 'B-NP': 0.6480490669450607, 'I-PP': 0.0, 'O': 0.14185045021532036, 'I-ADJP': 0.0, 'B-PRT': 0.0, 'B-CONJP': 0.00026099438862064463, 'B-INTJ': 0.0013049719431032234, 'I-VP': 0.0}, 'B-NP': {'B-ADVP': 0.00981034599384449, 'I-ADVP': 2.113843135928569e-06, 'I-NP': 0.6846759055703995, 'B-PP': 0.05800596949301586, 'B-ADJP': 0.0032151554097473536, 'I-INTJ': 2.113843135928569e-06, 'B-LST': 2.113843135928569e-06, 'I-UCP': 2.113843135928569e-06, 'I-SBAR': 2.113843135928569e-06, 'B-VP': 0.13029940474177293, 'I-CONJP': 2.113843135928569e-06, 'B-SBAR': 0.003405401291980925, 'B-UCP': 2.325227449521426e-05, 'B-NP': 0.028898349511279467, 'I-PP': 2.113843135928569e-06, 'O': 0.08096230594920012, 'I-ADJP': 2.113843135928569e-06, 'B-PRT': 0.0003614671762437853, 'B-CONJP': 8.666756857307133e-05, 'STOP': 0.00023463658808807117, 'B-INTJ': 2.113843135928569e-06, 'I-VP': 2.113843135928569e-06}, 'I-NP': {'B-ADVP': 0.01533341148714492, 'I-ADVP': 1.8317299590425182e-06, 'I-NP': 0.4066641999369885, 'B-PP': 0.1565048394305518, 'B-ADJP': 0.004104906838214283, 'I-INTJ': 1.8317299590425182e-06, 'B-LST': 1.8317299590425182e-06, 'I-UCP': 1.8317299590425182e-06, 'I-SBAR': 1.8317299590425182e-06, 'B-VP': 0.1349087432134405, 'I-CONJP': 1.8317299590425182e-06, 'B-SBAR': 0.006376251987427006, 'B-UCP': 1.8317299590425182e-06, 'B-NP': 0.04764512796465494, 'I-PP': 1.8317299590425182e-06, 'O': 0.22731951964713557, 'I-ADJP': 1.8317299590425182e-06, 'B-PRT': 0.00013005282709201878, 'B-CONJP': 0.0002033220254537195, 'STOP': 0.0007894756123473254, 'B-INTJ': 1.8317299590425182e-06, 'I-VP': 1.8317299590425182e-06}, 'B-VP': {'B-ADVP': 0.031215778176880284, 'I-ADVP': 5.475491699154584e-06, 'I-NP': 5.475491699154584e-06, 'B-PP': 0.0987285908274563, 'B-ADJP': 0.039209996057645975, 'I-INTJ': 5.475491699154584e-06, 'B-LST': 5.475491699154584e-06, 'I-UCP': 5.475491699154584e-06, 'I-SBAR': 5.475491699154584e-06, 'B-VP': 0.007233124534583205, 'I-CONJP': 5.475491699154584e-06, 'B-SBAR': 0.025576021726751063, 'B-UCP': 5.475491699154584e-06, 'B-NP': 0.34518047220640413, 'I-PP': 5.475491699154584e-06, 'O': 0.06740877830829207, 'I-ADJP': 5.475491699154584e-06, 'B-PRT': 0.011175478557974506, 'B-CONJP': 0.0001697402426737921, 'STOP': 6.023040869070043e-05, 'B-INTJ': 0.00011498532568224627, 'I-VP': 0.37387204870997415}, 'B-ADVP': {'B-ADVP': 0.016287284144427004, 'I-ADVP': 0.08693092621664052, 'I-NP': 2.8033191298497423e-05, 'B-PP': 0.17046983628616283, 'B-ADJP': 0.016567616057411978, 'I-INTJ': 2.8033191298497423e-05, 'B-LST': 2.8033191298497423e-05, 'I-UCP': 2.8033191298497423e-05, 'I-SBAR': 2.8033191298497423e-05, 'B-VP': 0.21588360618972866, 'I-CONJP': 2.8033191298497423e-05, 'B-SBAR': 0.016287284144427004, 'B-UCP': 2.8033191298497423e-05, 'B-NP': 0.21027696793002917, 'I-PP': 2.8033191298497423e-05, 'O': 0.2652220228750841, 'I-ADJP': 2.8033191298497423e-05, 'B-PRT': 0.0003083651042834717, 'B-CONJP': 0.0005886970172684459, 'STOP': 0.0008690289302534201, 'B-INTJ': 2.8033191298497423e-05, 'I-VP': 2.8033191298497423e-05}, 'B-ADJP': {'B-ADVP': 0.016027834816335844, 'I-ADVP': 5.703855806525211e-05, 'I-NP': 5.703855806525211e-05, 'B-PP': 0.2441820670773443, 'B-ADJP': 0.0011978097193702944, 'I-INTJ': 5.703855806525211e-05, 'B-LST': 5.703855806525211e-05, 'I-UCP': 5.703855806525211e-05, 'I-SBAR': 5.703855806525211e-05, 'B-VP': 0.11071184120465434, 'I-CONJP': 5.703855806525211e-05, 'B-SBAR': 0.03770248688113164, 'B-UCP': 5.703855806525211e-05, 'B-NP': 0.051962126397444665, 'I-PP': 5.703855806525211e-05, 'O': 0.25673054985169974, 'I-ADJP': 0.2795459730778006, 'B-PRT': 0.0006274241387177733, 'B-CONJP': 5.703855806525211e-05, 'STOP': 0.0006274241387177733, 'B-INTJ': 5.703855806525211e-05, 'I-VP': 5.703855806525211e-05}, 'I-ADJP': {'B-ADVP': 0.014057618882332522, 'I-ADVP': 0.00017355085039916696, 'I-NP': 0.00017355085039916696, 'B-PP': 0.28479694550503293, 'B-ADJP': 0.012322110378340852, 'I-INTJ': 0.00017355085039916696, 'B-LST': 0.00017355085039916696, 'I-UCP': 0.00017355085039916696, 'I-SBAR': 0.00017355085039916696, 'B-VP': 0.06959389101006595, 'I-CONJP': 0.00017355085039916696, 'B-SBAR': 0.0609163484901076, 'B-UCP': 0.00017355085039916696, 'B-NP': 0.0886844845539743, 'I-PP': 0.00017355085039916696, 'O': 0.31950711558486633, 'I-ADJP': 0.1459562651856994, 'B-PRT': 0.00017355085039916696, 'B-CONJP': 0.00017355085039916696, 'STOP': 0.0019090593543908366, 'B-INTJ': 0.00017355085039916696, 'I-VP': 0.00017355085039916696}, 'B-PP': {'B-ADVP': 0.0033226023970591434, 'I-ADVP': 5.437974463271921e-06, 'I-NP': 5.437974463271921e-06, 'B-PP': 0.018494551149587802, 'B-ADJP': 0.002615665716833794, 'I-INTJ': 5.437974463271921e-06, 'B-LST': 5.437974463271921e-06, 'I-UCP': 5.437974463271921e-06, 'I-SBAR': 5.437974463271921e-06, 'B-VP': 0.026597133099862964, 'I-CONJP': 5.437974463271921e-06, 'B-SBAR': 0.0009842733778522177, 'B-UCP': 5.437974463271921e-06, 'B-NP': 0.9279414003871836, 'I-PP': 0.011262045113436146, 'O': 0.008488678137167468, 'I-ADJP': 5.437974463271921e-06, 'B-PRT': 5.437974463271921e-06, 'B-CONJP': 5.437974463271921e-06, 'STOP': 0.00022295695299414872, 'B-INTJ': 5.437974463271921e-06, 'I-VP': 5.437974463271921e-06}, 'O': {'B-ADVP': 0.029198884151091974, 'I-ADVP': 4.188622027125516e-06, 'I-NP': 4.188622027125516e-06, 'B-PP': 0.05014199428671955, 'B-ADJP': 0.008758408658719454, 'I-INTJ': 4.188622027125516e-06, 'B-LST': 8.796106256963584e-05, 'I-UCP': 4.188622027125516e-06, 'I-SBAR': 4.188622027125516e-06, 'B-VP': 0.1150237494868938, 'I-CONJP': 4.188622027125516e-06, 'B-SBAR': 0.016130383426460365, 'B-UCP': 4.188622027125516e-06, 'B-NP': 0.34715718223018993, 'I-PP': 4.188622027125516e-06, 'O': 0.11351584555712861, 'I-ADJP': 4.188622027125516e-06, 'B-PRT': 4.607484229838068e-05, 'B-CONJP': 0.0010513441288085045, 'STOP': 0.31825569024302386, 'B-INTJ': 0.0005905957058246978, 'I-VP': 4.188622027125516e-06}, 'B-SBAR': {'B-ADVP': 0.008994319377235431, 'I-ADVP': 5.259835893120135e-05, 'I-NP': 5.259835893120135e-05, 'B-PP': 0.014254155270355565, 'B-ADJP': 0.003208499894803282, 'I-INTJ': 5.259835893120135e-05, 'B-LST': 0.0005785819482432148, 'I-UCP': 5.259835893120135e-05, 'I-SBAR': 0.025299810645907847, 'B-VP': 0.03844940037870818, 'I-CONJP': 5.259835893120135e-05, 'B-SBAR': 0.008468335787923418, 'B-UCP': 5.259835893120135e-05, 'B-NP': 0.8716074058489375, 'I-PP': 5.259835893120135e-05, 'O': 0.02845571218177993, 'I-ADJP': 5.259835893120135e-05, 'B-PRT': 5.259835893120135e-05, 'B-CONJP': 5.259835893120135e-05, 'STOP': 5.259835893120135e-05, 'B-INTJ': 5.259835893120135e-05, 'I-VP': 5.259835893120135e-05}, 'I-VP': {'B-ADVP': 0.037603826319726016, 'I-ADVP': 9.841357320001574e-06, 'I-NP': 9.841357320001574e-06, 'B-PP': 0.1483190961697437, 'B-ADJP': 0.029041845451324648, 'I-INTJ': 9.841357320001574e-06, 'B-LST': 9.841357320001574e-06, 'I-UCP': 9.841357320001574e-06, 'I-SBAR': 9.841357320001574e-06, 'B-VP': 0.008276581506121324, 'I-CONJP': 9.841357320001574e-06, 'B-SBAR': 0.01319726016612211, 'B-UCP': 9.841357320001574e-06, 'B-NP': 0.3552828406093768, 'I-PP': 9.841357320001574e-06, 'O': 0.05669605952052907, 'I-ADJP': 9.841357320001574e-06, 'B-PRT': 0.023333858205723732, 'B-CONJP': 0.00020666850372003307, 'STOP': 0.00010825493052001732, 'B-INTJ': 9.841357320001574e-06, 'I-VP': 0.3278254536865724}, 'I-ADVP': {'B-ADVP': 0.030394304490690034, 'I-ADVP': 0.14539978094194964, 'I-NP': 0.00027382256297918953, 'B-PP': 0.1536144578313253, 'B-ADJP': 0.016703176341730557, 'I-INTJ': 0.00027382256297918953, 'B-LST': 0.00027382256297918953, 'I-UCP': 0.00027382256297918953, 'I-SBAR': 0.00027382256297918953, 'B-VP': 0.05777656078860899, 'I-CONJP': 0.00027382256297918953, 'B-SBAR': 0.07968236582694414, 'B-UCP': 0.00027382256297918953, 'B-NP': 0.18647316538882802, 'I-PP': 0.00027382256297918953, 'O': 0.3261226725082147, 'I-ADJP': 0.00027382256297918953, 'B-PRT': 0.00027382256297918953, 'B-CONJP': 0.00027382256297918953, 'STOP': 0.00027382256297918953, 'B-INTJ': 0.00027382256297918953, 'I-VP': 0.00027382256297918953}, 'B-PRT': {'B-ADVP': 0.029987239472564865, 'I-ADVP': 0.0002126754572522331, 'I-NP': 0.0002126754572522331, 'B-PP': 0.2405359421522756, 'B-ADJP': 0.0023394300297745643, 'I-INTJ': 0.0002126754572522331, 'B-LST': 0.0002126754572522331, 'I-UCP': 0.0002126754572522331, 'I-SBAR': 0.0002126754572522331, 'B-VP': 0.04274776690769885, 'I-CONJP': 0.0002126754572522331, 'B-SBAR': 0.019353466609953213, 'B-UCP': 0.0002126754572522331, 'B-NP': 0.5042535091450446, 'I-PP': 0.0002126754572522331, 'O': 0.1575925138239047, 'I-ADJP': 0.0002126754572522331, 'B-PRT': 0.0002126754572522331, 'B-CONJP': 0.0002126754572522331, 'STOP': 0.0002126754572522331, 'B-INTJ': 0.0002126754572522331, 'I-VP': 0.0002126754572522331}, 'I-PP': {'B-ADVP': 0.004884547069271759, 'I-ADVP': 0.00044404973357015993, 'I-NP': 0.00044404973357015993, 'B-PP': 0.06261101243339254, 'B-ADJP': 0.004884547069271759, 'I-INTJ': 0.00044404973357015993, 'B-LST': 0.00044404973357015993, 'I-UCP': 0.00044404973357015993, 'I-SBAR': 0.00044404973357015993, 'B-VP': 0.03152753108348135, 'I-CONJP': 0.00044404973357015993, 'B-SBAR': 0.00044404973357015993, 'B-UCP': 0.00044404973357015993, 'B-NP': 0.7775310834813499, 'I-PP': 0.07149200710479575, 'O': 0.040408525754884544, 'I-ADJP': 0.00044404973357015993, 'B-PRT': 0.00044404973357015993, 'B-CONJP': 0.00044404973357015993, 'STOP': 0.00044404973357015993, 'B-INTJ': 0.00044404973357015993, 'I-VP': 0.00044404973357015993}, 'B-CONJP': {'B-ADVP': 0.001953125, 'I-ADVP': 0.001953125, 'I-NP': 0.001953125, 'B-PP': 0.001953125, 'B-ADJP': 0.001953125, 'I-INTJ': 0.001953125, 'B-LST': 0.001953125, 'I-UCP': 0.001953125, 'I-SBAR': 0.001953125, 'B-VP': 0.001953125, 'I-CONJP': 0.900390625, 'B-SBAR': 0.001953125, 'B-UCP': 0.001953125, 'B-NP': 0.001953125, 'I-PP': 0.001953125, 'O': 0.060546875, 'I-ADJP': 0.001953125, 'B-PRT': 0.001953125, 'B-CONJP': 0.001953125, 'STOP': 0.001953125, 'B-INTJ': 0.001953125, 'I-VP': 0.001953125}, 'I-CONJP': {'B-ADVP': 0.0015105740181268882, 'I-ADVP': 0.0015105740181268882, 'I-NP': 0.0015105740181268882, 'B-PP': 0.10725075528700906, 'B-ADJP': 0.0015105740181268882, 'I-INTJ': 0.0015105740181268882, 'B-LST': 0.0015105740181268882, 'I-UCP': 0.0015105740181268882, 'I-SBAR': 0.0015105740181268882, 'B-VP': 0.1525679758308157, 'I-CONJP': 0.27341389728096677, 'B-SBAR': 0.0015105740181268882, 'B-UCP': 0.0015105740181268882, 'B-NP': 0.4244712990936556, 'I-PP': 0.0015105740181268882, 'O': 0.01661631419939577, 'I-ADJP': 0.0015105740181268882, 'B-PRT': 0.0015105740181268882, 'B-CONJP': 0.0015105740181268882, 'STOP': 0.0015105740181268882, 'B-INTJ': 0.0015105740181268882, 'I-VP': 0.0015105740181268882}, 'B-INTJ': {'B-ADVP': 0.03900709219858156, 'I-ADVP': 0.003546099290780142, 'I-NP': 0.003546099290780142, 'B-PP': 0.03900709219858156, 'B-ADJP': 0.003546099290780142, 'I-INTJ': 0.18085106382978722, 'B-LST': 0.003546099290780142, 'I-UCP': 0.003546099290780142, 'I-SBAR': 0.003546099290780142, 'B-VP': 0.07446808510638299, 'I-CONJP': 0.003546099290780142, 'B-SBAR': 0.003546099290780142, 'B-UCP': 0.003546099290780142, 'B-NP': 0.03900709219858156, 'I-PP': 0.003546099290780142, 'O': 0.5709219858156029, 'I-ADJP': 0.003546099290780142, 'B-PRT': 0.003546099290780142, 'B-CONJP': 0.003546099290780142, 'STOP': 0.003546099290780142, 'B-INTJ': 0.003546099290780142, 'I-VP': 0.003546099290780142}, 'I-INTJ': {'B-ADVP': 0.010869565217391306, 'I-ADVP': 0.010869565217391306, 'I-NP': 0.010869565217391306, 'B-PP': 0.010869565217391306, 'B-ADJP': 0.010869565217391306, 'I-INTJ': 0.2282608695652174, 'B-LST': 0.010869565217391306, 'I-UCP': 0.010869565217391306, 'I-SBAR': 0.010869565217391306, 'B-VP': 0.010869565217391306, 'I-CONJP': 0.010869565217391306, 'B-SBAR': 0.010869565217391306, 'B-UCP': 0.010869565217391306, 'B-NP': 0.010869565217391306, 'I-PP': 0.010869565217391306, 'O': 0.5543478260869565, 'I-ADJP': 0.010869565217391306, 'B-PRT': 0.010869565217391306, 'B-CONJP': 0.010869565217391306, 'STOP': 0.010869565217391306, 'B-INTJ': 0.010869565217391306, 'I-VP': 0.010869565217391306}, 'I-SBAR': {'B-ADVP': 0.00199203187250996, 'I-ADVP': 0.00199203187250996, 'I-NP': 0.00199203187250996, 'B-PP': 0.021912350597609563, 'B-ADJP': 0.00199203187250996, 'I-INTJ': 0.00199203187250996, 'B-LST': 0.00199203187250996, 'I-UCP': 0.00199203187250996, 'I-SBAR': 0.00199203187250996, 'B-VP': 0.021912350597609563, 'I-CONJP': 0.00199203187250996, 'B-SBAR': 0.00199203187250996, 'B-UCP': 0.00199203187250996, 'B-NP': 0.9183266932270916, 'I-PP': 0.00199203187250996, 'O': 0.00199203187250996, 'I-ADJP': 0.00199203187250996, 'B-PRT': 0.00199203187250996, 'B-CONJP': 0.00199203187250996, 'STOP': 0.00199203187250996, 'B-INTJ': 0.00199203187250996, 'I-VP': 0.00199203187250996}, 'B-UCP': {'B-ADVP': 0.03125, 'I-ADVP': 0.03125, 'I-NP': 0.03125, 'B-PP': 0.03125, 'B-ADJP': 0.03125, 'I-INTJ': 0.03125, 'B-LST': 0.03125, 'I-UCP': 0.34375, 'I-SBAR': 0.03125, 'B-VP': 0.03125, 'I-CONJP': 0.03125, 'B-SBAR': 0.03125, 'B-UCP': 0.03125, 'B-NP': 0.03125, 'I-PP': 0.03125, 'O': 0.03125, 'I-ADJP': 0.03125, 'B-PRT': 0.03125, 'B-CONJP': 0.03125, 'STOP': 0.03125, 'B-INTJ': 0.03125, 'I-VP': 0.03125}, 'I-UCP': {'B-ADVP': 0.016129032258064516, 'I-ADVP': 0.016129032258064516, 'I-NP': 0.016129032258064516, 'B-PP': 0.016129032258064516, 'B-ADJP': 0.016129032258064516, 'I-INTJ': 0.016129032258064516, 'B-LST': 0.016129032258064516, 'I-UCP': 0.5, 'I-SBAR': 0.016129032258064516, 'B-VP': 0.016129032258064516, 'I-CONJP': 0.016129032258064516, 'B-SBAR': 0.016129032258064516, 'B-UCP': 0.016129032258064516, 'B-NP': 0.1774193548387097, 'I-PP': 0.016129032258064516, 'O': 0.016129032258064516, 'I-ADJP': 0.016129032258064516, 'B-PRT': 0.016129032258064516, 'B-CONJP': 0.016129032258064516, 'STOP': 0.016129032258064516, 'B-INTJ': 0.016129032258064516, 'I-VP': 0.016129032258064516}, 'B-LST': {'B-ADVP': 0.007575757575757577, 'I-ADVP': 0.007575757575757577, 'I-NP': 0.007575757575757577, 'B-PP': 0.007575757575757577, 'B-ADJP': 0.007575757575757577, 'I-INTJ': 0.007575757575757577, 'B-LST': 0.007575757575757577, 'I-UCP': 0.007575757575757577, 'I-SBAR': 0.007575757575757577, 'B-VP': 0.007575757575757577, 'I-CONJP': 0.007575757575757577, 'B-SBAR': 0.007575757575757577, 'B-UCP': 0.007575757575757577, 'B-NP': 0.007575757575757577, 'I-PP': 0.007575757575757577, 'O': 0.8409090909090909, 'I-ADJP': 0.007575757575757577, 'B-PRT': 0.007575757575757577, 'B-CONJP': 0.007575757575757577, 'STOP': 0.007575757575757577, 'B-INTJ': 0.007575757575757577, 'I-VP': 0.007575757575757577}})\n"
     ]
    }
   ],
   "source": [
    "def compute_transition_parameters(train_file_path, smoothing=0.1):\n",
    "    \"\"\"\n",
    "    Computes transition probabilities with smoothing and proper STOP/START handling.\n",
    "    \n",
    "    Args:\n",
    "        train_file_path: Path to training file (word-tag pairs separated by whitespace)\n",
    "        smoothing: Laplace smoothing factor (default: 0.1)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of dictionaries: transition_parameters[prev_tag][tag] = probability\n",
    "    \"\"\"\n",
    "    transition_counts = defaultdict(lambda: defaultdict(float))\n",
    "    prev_tag_counts = defaultdict(float)\n",
    "    all_tags = set()\n",
    "\n",
    "    # Initialize with smoothing for all possible transitions\n",
    "    with open(train_file_path, 'r', encoding='utf-8') as file:\n",
    "        prev_tag = 'START'\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:  # Non-empty line (word-tag pair)\n",
    "                try:\n",
    "                    _, tag = line.split()\n",
    "                    transition_counts[prev_tag][tag] += 1\n",
    "                    prev_tag_counts[prev_tag] += 1\n",
    "                    all_tags.add(tag)\n",
    "                    prev_tag = tag\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping invalid line: {line}\")\n",
    "            else:  # Empty line (sentence boundary)\n",
    "                transition_counts[prev_tag]['STOP'] += 1\n",
    "                prev_tag_counts[prev_tag] += 1\n",
    "                prev_tag = 'START'  # Reset for next sentence\n",
    "        all_tags.add('STOP')\n",
    "\n",
    "    # Apply Laplace smoothing and normalize\n",
    "    transition_parameters = defaultdict(dict)\n",
    "    for prev_tag in transition_counts:\n",
    "        total = prev_tag_counts[prev_tag] + smoothing * len(all_tags)\n",
    "        for tag in all_tags:\n",
    "            count = transition_counts[prev_tag].get(tag, 0) + smoothing\n",
    "            transition_parameters[prev_tag][tag] = count / total\n",
    "\n",
    "    # Ensure START -> first tag is properly initialized\n",
    "    transition_parameters['START'] = {\n",
    "        tag: transition_counts['START'].get(tag, 0) / prev_tag_counts['START']\n",
    "        for tag in all_tags if tag != 'STOP'\n",
    "    }\n",
    "\n",
    "    return transition_parameters\n",
    "\n",
    "transition_parameters = compute_transition_parameters(train_file_path)\n",
    "print(transition_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def viterbi(sentence, transition_params, emission_params, all_tags):\n",
    "    \"\"\"\n",
    "    Viterbi algorithm with robust probability handling\n",
    "    \"\"\"\n",
    "    n = len(sentence)\n",
    "    viterbi_matrix = defaultdict(dict)\n",
    "    backpointer = defaultdict(dict)\n",
    "    \n",
    "    # Small epsilon value to avoid log(0)\n",
    "    EPSILON = 1e-10\n",
    "    \n",
    "    # Initialize first step\n",
    "    for tag in all_tags:\n",
    "        # Handle emission probability\n",
    "        emission_prob = emission_params[tag].get(sentence[0], EPSILON)\n",
    "        \n",
    "        # Handle transition probability\n",
    "        trans_prob = transition_params['START'].get(tag, EPSILON)\n",
    "        \n",
    "        # Calculate log probabilities safely\n",
    "        if emission_prob <= 0:\n",
    "            emission_prob = EPSILON\n",
    "        if trans_prob <= 0:\n",
    "            trans_prob = EPSILON\n",
    "            \n",
    "        viterbi_matrix[0][tag] = math.log(trans_prob) + math.log(emission_prob)\n",
    "        backpointer[0][tag] = 'START'\n",
    "    \n",
    "    # Recursion\n",
    "    for t in range(1, n):\n",
    "        word = sentence[t]\n",
    "        for current_tag in all_tags:\n",
    "            max_prob = -float('inf')\n",
    "            best_prev_tag = None\n",
    "            \n",
    "            for prev_tag in all_tags:\n",
    "                # Get probabilities safely\n",
    "                trans_prob = transition_params[prev_tag].get(current_tag, EPSILON)\n",
    "                emission_prob = emission_params[current_tag].get(word, EPSILON)\n",
    "                \n",
    "                if trans_prob <= 0:\n",
    "                    trans_prob = EPSILON\n",
    "                if emission_prob <= 0:\n",
    "                    emission_prob = EPSILON\n",
    "                \n",
    "                current_prob = (viterbi_matrix[t-1][prev_tag] + \n",
    "                               math.log(trans_prob) + \n",
    "                               math.log(emission_prob))\n",
    "                \n",
    "                if current_prob > max_prob:\n",
    "                    max_prob = current_prob\n",
    "                    best_prev_tag = prev_tag\n",
    "            \n",
    "            viterbi_matrix[t][current_tag] = max_prob\n",
    "            backpointer[t][current_tag] = best_prev_tag\n",
    "    \n",
    "    # Termination\n",
    "    max_prob = -float('inf')\n",
    "    best_last_tag = None\n",
    "    for tag in all_tags:\n",
    "        stop_prob = viterbi_matrix[n-1][tag] + math.log(transition_params[tag].get('STOP', EPSILON))\n",
    "        if stop_prob > max_prob:\n",
    "            max_prob = stop_prob\n",
    "            best_last_tag = tag\n",
    "    \n",
    "    # Backtrace\n",
    "    tags = [best_last_tag]\n",
    "    for t in range(n-1, 0, -1):\n",
    "        tags.append(backpointer[t][tags[-1]])\n",
    "    tags.reverse()\n",
    "    \n",
    "    return tags\n",
    "\n",
    "def run_viterbi_on_dev_set(dev_in_path, output_path, transition_params, emission_params, all_tags):\n",
    "    \"\"\"\n",
    "    Runs Viterbi on development set and writes predictions\n",
    "    \"\"\"\n",
    "    with open(dev_in_path, 'r', encoding='utf-8') as fin, \\\n",
    "         open(output_path, 'w', encoding='utf-8') as fout:\n",
    "        \n",
    "        current_sentence = []\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                current_sentence.append(line)\n",
    "            else:\n",
    "                if current_sentence:\n",
    "                    predicted_tags = viterbi(current_sentence, transition_params, \n",
    "                                           emission_params, all_tags)\n",
    "                    for word, tag in zip(current_sentence, predicted_tags):\n",
    "                        fout.write(f\"{word} {tag}\\n\")\n",
    "                    fout.write(\"\\n\")\n",
    "                current_sentence = []\n",
    "        \n",
    "        # Handle last sentence if file doesn't end with newline\n",
    "        if current_sentence:\n",
    "            predicted_tags = viterbi(current_sentence, transition_params, \n",
    "                                   emission_params, all_tags)\n",
    "            for word, tag in zip(current_sentence, predicted_tags):\n",
    "                fout.write(f\"{word} {tag}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8947\n",
      "Recall:    0.8542\n",
      "F1 Score:  0.8740\n",
      "True Positives:  697\n",
      "False Positives: 82\n",
      "False Negatives: 119\n"
     ]
    }
   ],
   "source": [
    "train_file = \"EN/train\"\n",
    "dev_in_file = \"EN/dev.in\"\n",
    "dev_out_file = \"EN/dev.p2.out\"\n",
    "gold_file = \"EN/dev.out\"\n",
    "    \n",
    "    \n",
    "emission_params = compute_emission_parameters(train_file)\n",
    "transition_params = compute_transition_parameters(train_file)\n",
    "    \n",
    "\n",
    "all_tags = set()\n",
    "with open(train_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            try:\n",
    "                _, tag = line.split()\n",
    "                all_tags.add(tag)\n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "# Run Viterbi on dev set\n",
    "run_viterbi_on_dev_set(dev_in_file, dev_out_file, transition_params, emission_params, all_tags)\n",
    "    \n",
    "# Evaluate\n",
    "metrics = evaluate(gold_file, dev_out_file)\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict, namedtuple\n",
    "\n",
    "# Data structure to store multiple paths\n",
    "PathInfo = namedtuple('PathInfo', ['prob', 'tags'])\n",
    "\n",
    "def topk_viterbi(sentence, transition_params, emission_params, all_tags, k=4):\n",
    "    \"\"\"\n",
    "    Finds top-k best sequences using modified Viterbi algorithm\n",
    "    \n",
    "    Args:\n",
    "        sentence: List of words\n",
    "        transition_params: Learned transition probabilities\n",
    "        emission_params: Learned emission probabilities\n",
    "        all_tags: Set of all possible tags\n",
    "        k: Number of top sequences to find\n",
    "    \n",
    "    Returns:\n",
    "        List of PathInfo objects sorted by probability (descending)\n",
    "    \"\"\"\n",
    "    n = len(sentence)\n",
    "    EPSILON = 1e-10\n",
    "    \n",
    "    # Initialize data structures\n",
    "    # viterbi[t][tag] = list of top-k (prob, backpointer) up to position t\n",
    "    viterbi = [defaultdict(list) for _ in range(n)]\n",
    "    backpointers = [defaultdict(list) for _ in range(n)]\n",
    "    \n",
    "    # Initialization step\n",
    "    for tag in all_tags:\n",
    "        emission_prob = emission_params[tag].get(sentence[0], EPSILON)\n",
    "        trans_prob = transition_params['START'].get(tag, EPSILON)\n",
    "        prob = math.log(max(trans_prob, EPSILON)) + math.log(max(emission_prob, EPSILON))\n",
    "        viterbi[0][tag].append((prob, 'START'))\n",
    "    \n",
    "    # Recursion step\n",
    "    for t in range(1, n):\n",
    "        word = sentence[t]\n",
    "        for current_tag in all_tags:\n",
    "            # Collect all possible transitions\n",
    "            candidates = []\n",
    "            emission_prob = emission_params[current_tag].get(word, EPSILON)\n",
    "            \n",
    "            for prev_tag in all_tags:\n",
    "                if not viterbi[t-1][prev_tag]:\n",
    "                    continue\n",
    "                    \n",
    "                trans_prob = transition_params[prev_tag].get(current_tag, EPSILON)\n",
    "                \n",
    "                # Combine with previous paths\n",
    "                for prev_prob, _ in viterbi[t-1][prev_tag]:\n",
    "                    new_prob = prev_prob + math.log(max(trans_prob, EPSILON)) + math.log(max(emission_prob, EPSILON))\n",
    "                    candidates.append((new_prob, prev_tag))\n",
    "            \n",
    "            # Keep top-k paths\n",
    "            candidates.sort(reverse=True, key=lambda x: x[0])\n",
    "            viterbi[t][current_tag] = candidates[:k]\n",
    "    \n",
    "    # Termination step\n",
    "    final_candidates = []\n",
    "    for tag in all_tags:\n",
    "        if not viterbi[n-1][tag]:\n",
    "            continue\n",
    "            \n",
    "        for prob, prev_tag in viterbi[n-1][tag]:\n",
    "            stop_prob = prob + math.log(max(transition_params[tag].get('STOP', EPSILON), EPSILON))\n",
    "            final_candidates.append((stop_prob, tag, prev_tag))\n",
    "    \n",
    "    final_candidates.sort(reverse=True, key=lambda x: x[0])\n",
    "    top_k_candidates = final_candidates[:k]\n",
    "    \n",
    "    # Backtrace to get sequences\n",
    "    results = []\n",
    "    for prob, last_tag, prev_tag in top_k_candidates:\n",
    "        tags = [last_tag]\n",
    "        current_tag = last_tag\n",
    "        prev_tag_ptr = prev_tag\n",
    "        \n",
    "        # Backtrace through the sentence\n",
    "        for t in range(n-1, 0, -1):\n",
    "            # Find which of the top-k paths we're following\n",
    "            for candidate_prob, candidate_prev in viterbi[t][current_tag]:\n",
    "                if math.isclose(candidate_prob + math.log(max(transition_params[candidate_prev].get(current_tag, EPSILON), EPSILON)), \n",
    "                               prob - math.log(max(transition_params[last_tag].get('STOP', EPSILON), EPSILON)), \n",
    "                               abs_tol=1e-6):\n",
    "                    tags.append(candidate_prev)\n",
    "                    current_tag = candidate_prev\n",
    "                    prob = candidate_prob\n",
    "                    break\n",
    "        \n",
    "        tags.reverse()\n",
    "        results.append(PathInfo(prob=math.exp(prob), tags=tags))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_topk_viterbi(dev_in_path, output_path, transition_params, emission_params, all_tags):\n",
    "    \"\"\"\n",
    "    Runs top-k Viterbi on development set and writes predictions (4th best)\n",
    "    \"\"\"\n",
    "    with open(dev_in_path, 'r', encoding='utf-8') as fin, \\\n",
    "         open(output_path, 'w', encoding='utf-8') as fout:\n",
    "        \n",
    "        current_sentence = []\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                current_sentence.append(line)\n",
    "            else:\n",
    "                if current_sentence:\n",
    "                    top_sequences = topk_viterbi(current_sentence, transition_params, \n",
    "                                               emission_params, all_tags, k=4)\n",
    "                    if len(top_sequences) >= 4:\n",
    "                        # Write the 4th best sequence\n",
    "                        for word, tag in zip(current_sentence, top_sequences[3].tags):\n",
    "                            fout.write(f\"{word} {tag}\\n\")\n",
    "                    else:\n",
    "                        # Fallback to best sequence if not enough candidates\n",
    "                        for word, tag in zip(current_sentence, top_sequences[-1].tags):\n",
    "                            fout.write(f\"{word} {tag}\\n\")\n",
    "                    fout.write(\"\\n\")\n",
    "                current_sentence = []\n",
    "        \n",
    "        if current_sentence:\n",
    "            top_sequences = topk_viterbi(current_sentence, transition_params, \n",
    "                                       emission_params, all_tags, k=4)\n",
    "            if len(top_sequences) >= 4:\n",
    "                for word, tag in zip(current_sentence, top_sequences[3].tags):\n",
    "                    fout.write(f\"{word} {tag}\\n\")\n",
    "            else:\n",
    "                for word, tag in zip(current_sentence, top_sequences[-1].tags):\n",
    "                    fout.write(f\"{word} {tag}\\n\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for 4th-best sequences:\n",
      "Precision: 1.0000\n",
      "Recall:    0.2500\n",
      "F1 Score:  0.4000\n",
      "True Positives:  2\n",
      "False Positives: 0\n",
      "False Negatives: 6\n"
     ]
    }
   ],
   "source": [
    "run_topk_viterbi(dev_in_file, dev_out_file, transition_params, emission_params, all_tags)\n",
    "    \n",
    "    # Evaluate\n",
    "metrics = evaluate(gold_file, dev_out_file)\n",
    "print(\"Evaluation for 4th-best sequences:\")\n",
    "print_metrics(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
