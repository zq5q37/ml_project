{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50.007 Machine Learning, Spring 2025\n",
    "# Design Project\n",
    "\n",
    "Due 27 Apr 2025, 5:00pm\n",
    "\n",
    "By: Aishwarya Iyer (1007141) and Khoo Zi Qi (1006984)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 (30points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"EN/train\"\n",
    "# print(\"File exists:\", os.path.exists(train_file))\n",
    "\n",
    "gold_file = \"EN/dev.out\"\n",
    "\n",
    "dev_in_file = 'EN/dev.in'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that estimates the emission parameters from the training set using MLE (maximum likelihood estimation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Computes emission parameters for an HMM: e(x|y) = Count(y → x) / Count(y)\n",
    "where:\n",
    "- x: observed word\n",
    "- y: corresponding tag (e.g., 'B-NP', 'I-VP', 'O')\n",
    "\"\"\"\n",
    "# Use defaultdict to automatically handles missing keys\n",
    "from collections import defaultdict\n",
    "\n",
    "def compute_emission_parameters(train_file_path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        train_file_path: Path to training file (word-tag pairs separated by whitespace)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of dictionaries: emission_parameters[tag][word] = probability\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize counters:\n",
    "    # - emission_counts[tag][word] = times word appears with tag\n",
    "    # - tag_counts[tag] = total occurrences of tag\n",
    "    emission_counts = defaultdict(lambda: defaultdict(int))\n",
    "    tag_counts = defaultdict(int)\n",
    "\n",
    "    # Count word-tag co-occurrences and tag frequencies\n",
    "    with open(train_file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:  # Skip empty lines\n",
    "                try:\n",
    "                    word, tag = line.split()  # Split by any whitespace\n",
    "                    emission_counts[tag][word] += 1\n",
    "                    tag_counts[tag] += 1\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping invalid line: {line}\")\n",
    "\n",
    "    # Calculate emission probabilities\n",
    "    emission_parameters = defaultdict(dict)\n",
    "    for tag in emission_counts:\n",
    "        total_tag_occurrences = tag_counts[tag]\n",
    "        for word in emission_counts[tag]:\n",
    "            emission_parameters[tag][word] = (\n",
    "                emission_counts[tag][word] / total_tag_occurrences\n",
    "            )\n",
    "    \n",
    "    return emission_parameters\n",
    "\n",
    "emission_parameters = compute_emission_parameters(train_file)\n",
    "# print(emission_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use smoothing\n",
    "- Identify words that appear less than 3 times\n",
    "- Replace those words with #UNK#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_emission_parameters_smoothing(train_file_path, k=3):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        train_file_path: Path to training file (word-tag pairs separated by whitespace)\n",
    "        k: minimum count of word. If word count less than k, replace word with #UNK#.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of dictionaries: emission_parameters[tag][word] = probability\n",
    "    \"\"\"\n",
    "    \n",
    "    word_counts = defaultdict(int)\n",
    "    with open(train_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:  # not an empty line\n",
    "                word, tag = line.split()\n",
    "                word_counts[word] += 1\n",
    "    \n",
    "    # Identify rare words\n",
    "    rare_words = {word for word, count in word_counts.items() if count < k}\n",
    "\n",
    "    # Initialize counters:\n",
    "    # - emission_counts[tag][word] = times word appears with tag\n",
    "    # - tag_counts[tag] = total occurrences of tag\n",
    "    emission_counts = defaultdict(lambda: defaultdict(int))\n",
    "    tag_counts = defaultdict(int)\n",
    "\n",
    "    # Count word-tag co-occurrences and tag frequencies\n",
    "    with open(train_file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:  # Skip empty lines\n",
    "                try:\n",
    "                    word, tag = line.split()  # Split by any whitespace\n",
    "                    processed_word = word if word not in rare_words else '#UNK#' #modify training set\n",
    "                    emission_counts[tag][processed_word] += 1\n",
    "                    tag_counts[tag] += 1\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping invalid line: {line}\")\n",
    "\n",
    "    # Calculate emission probabilities\n",
    "    emission_parameters = defaultdict(dict)\n",
    "    for tag in emission_counts:\n",
    "        total_tag_occurrences = tag_counts[tag]\n",
    "        for word in emission_counts[tag]:\n",
    "            emission_parameters[tag][word] = (\n",
    "                emission_counts[tag][word] / total_tag_occurrences\n",
    "            )\n",
    "    \n",
    "    return emission_parameters\n",
    "\n",
    "emission_parameters_smoothing = compute_emission_parameters_smoothing(train_file, k = 3)\n",
    "# print(emission_parameters_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a simple system that produces the tag\n",
    "y∗= arg maxy e(x|y)\n",
    "for each word x in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tags(dev_in_file_path, emission_parameters, unknown_tag='O'):\n",
    "    \"\"\"\n",
    "    Predicts tags for a sentence using emission probabilities.\n",
    "    \n",
    "    Args:\n",
    "        sentence: List of words to tag\n",
    "        emission_params: Dictionary from compute_emission_parameters()\n",
    "        unknown_tag: Default tag for unseen words\n",
    "    \n",
    "    Returns:\n",
    "        List of (word, predicted_tag) tuples\n",
    "    \"\"\"\n",
    "    predicted = []\n",
    "    \n",
    "    with open(dev_in_file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            word = line.strip()\n",
    "            if word:  # Skip empty lines, each line has one word\n",
    "                max_prob = -1\n",
    "                best_tag = unknown_tag  # Default fallback\n",
    "                try:\n",
    "                    # Find tag with highest emission probability for this word\n",
    "                    for tag in emission_parameters:\n",
    "                        if word in emission_parameters[tag]:\n",
    "                            if emission_parameters[tag][word] > max_prob:\n",
    "                                max_prob = emission_parameters[tag][word]\n",
    "                                best_tag = tag\n",
    "                    \n",
    "                    predicted.append((word, best_tag))\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping invalid line: {line}\")         \n",
    "    return predicted\n",
    "\n",
    "predicted_list = predict_tags(dev_in_file, emission_parameters_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn these parameters with train, and evaluate your system on the development set dev.in for\n",
    "each of the dataset. Write your output to dev.p2.out. (There's a typo in the project brief? Should be dev.p1.out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions(predicted_list, output_file_path):\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as fout:\n",
    "        for word, tag in predicted_list:\n",
    "            fout.write(f\"{word} {tag}\\n\")\n",
    "\n",
    "write_predictions(predicted_list, 'EN/dev.p1.out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your outputs and the gold-standard outputs in dev.out and report the precision, recall and F scores of such a baseline system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_chunks(tag_sequence):\n",
    "#     \"\"\"Convert tag sequence to list of (start_idx, end_idx, chunk_type) tuples\"\"\"\n",
    "#     chunks = []\n",
    "#     current_chunk = None\n",
    "    \n",
    "#     for i, tag in enumerate(tag_sequence):\n",
    "#         if tag.startswith('B-'):\n",
    "#             if current_chunk:\n",
    "#                 chunks.append(current_chunk)\n",
    "#             current_chunk = (i, i+1, tag[2:])\n",
    "#         elif tag.startswith('I-'):\n",
    "#             if current_chunk and current_chunk[2] == tag[2:]:\n",
    "#                 current_chunk = (current_chunk[0], i+1, current_chunk[2])\n",
    "#             else:\n",
    "#                 # Invalid transition (O → I), treat as B-\n",
    "#                 if current_chunk:\n",
    "#                     chunks.append(current_chunk)\n",
    "#                 current_chunk = (i, i+1, tag[2:])\n",
    "#         else:  # O\n",
    "#             if current_chunk:\n",
    "#                 chunks.append(current_chunk)\n",
    "#             current_chunk = None\n",
    "    \n",
    "#     if current_chunk:\n",
    "#         chunks.append(current_chunk)\n",
    "    \n",
    "#     return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(gold_file, pred_file):\n",
    "#     \"\"\"Calculate precision, recall and F1\"\"\"\n",
    "#     gold_chunks = []\n",
    "#     pred_chunks = []\n",
    "    \n",
    "#     # Read both files simultaneously\n",
    "#     with open(gold_file, 'r', encoding='utf-8') as fgold, \\\n",
    "#          open(pred_file, 'r', encoding='utf-8') as fpred:\n",
    "        \n",
    "#         gold_sentence = []\n",
    "#         pred_sentence = []\n",
    "        \n",
    "#         for gold_line, pred_line in zip(fgold, fpred):\n",
    "#             gold_line = gold_line.strip()\n",
    "#             pred_line = pred_line.strip()\n",
    "            \n",
    "#             if gold_line and pred_line:\n",
    "#                 # Get tags (assuming format: word Tag)\n",
    "#                 gold_tag = gold_line.split()[1]\n",
    "#                 pred_tag = pred_line.split()[1]\n",
    "#                 gold_sentence.append(gold_tag)\n",
    "#                 pred_sentence.append(pred_tag)\n",
    "#             else:\n",
    "#                 # End of sentence\n",
    "#                 if gold_sentence and pred_sentence:\n",
    "#                     gold_chunks.extend(extract_chunks(gold_sentence))\n",
    "#                     pred_chunks.extend(extract_chunks(pred_sentence))\n",
    "#                 gold_sentence = []\n",
    "#                 pred_sentence = []\n",
    "    \n",
    "#     # Calculate metrics\n",
    "#     gold_set = set(gold_chunks)\n",
    "#     pred_set = set(pred_chunks)\n",
    "    \n",
    "#     tp = len(gold_set & pred_set)  # True positives\n",
    "#     fp = len(pred_set - gold_set)  # False positives\n",
    "#     fn = len(gold_set - pred_set)  # False negatives\n",
    "\n",
    "\n",
    "    \n",
    "#     precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "#     recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "#     f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "#     return {\n",
    "#         'precision': precision,\n",
    "#         'recall': recall,\n",
    "#         'f1': f1\n",
    "#     }\n",
    "\n",
    "# def print_metrics(metrics):\n",
    "#     \"\"\"Pretty-print evaluation metrics\"\"\"\n",
    "#     print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "#     print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "#     print(f\"F1 Score:  {metrics['f1']:.4f}\")\n",
    "\n",
    "# metrics = evaluate(gold_file, 'EN/dev.p1.out')\n",
    "# print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: python\n"
     ]
    }
   ],
   "source": [
    "!python EvalScript/evalResult.py EN/dev.out EN/dev.p1.out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 (25 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transition_parameters(train_file_path, smoothing=0.1):\n",
    "    \"\"\"\n",
    "    Computes transition probabilities with smoothing and proper STOP/START handling.\n",
    "    \n",
    "    Args:\n",
    "        train_file_path: Path to training file (word-tag pairs separated by whitespace)\n",
    "        smoothing: Laplace smoothing factor (default: 0.1)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of dictionaries: transition_parameters[prev_tag][tag] = probability\n",
    "    \"\"\"\n",
    "    transition_counts = defaultdict(lambda: defaultdict(float))\n",
    "    prev_tag_counts = defaultdict(float)\n",
    "    all_tags = set()\n",
    "\n",
    "    # Initialize with smoothing for all possible transitions\n",
    "    with open(train_file_path, 'r', encoding='utf-8') as file:\n",
    "        prev_tag = 'START'\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:  # Non-empty line (word-tag pair)\n",
    "                try:\n",
    "                    _, tag = line.split()\n",
    "                    transition_counts[prev_tag][tag] += 1\n",
    "                    prev_tag_counts[prev_tag] += 1\n",
    "                    all_tags.add(tag)\n",
    "                    prev_tag = tag\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping invalid line: {line}\")\n",
    "            else:  # Empty line (sentence boundary)\n",
    "                transition_counts[prev_tag]['STOP'] += 1\n",
    "                prev_tag_counts[prev_tag] += 1\n",
    "                prev_tag = 'START'  # Reset for next sentence\n",
    "        all_tags.add('STOP')\n",
    "\n",
    "    # Apply Laplace smoothing and normalize\n",
    "    transition_parameters = defaultdict(dict)\n",
    "    for prev_tag in transition_counts:\n",
    "        total = prev_tag_counts[prev_tag] + smoothing * len(all_tags)\n",
    "        for tag in all_tags:\n",
    "            count = transition_counts[prev_tag].get(tag, 0) + smoothing\n",
    "            transition_parameters[prev_tag][tag] = count / total\n",
    "\n",
    "    # Ensure START -> first tag is properly initialized\n",
    "    transition_parameters['START'] = {\n",
    "        tag: transition_counts['START'].get(tag, 0) / prev_tag_counts['START']\n",
    "        for tag in all_tags if tag != 'STOP'\n",
    "    }\n",
    "\n",
    "    return transition_parameters\n",
    "\n",
    "transition_parameters = compute_transition_parameters(train_file)\n",
    "# print(transition_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def viterbi(sentence, transition_params, emission_params, all_tags):\n",
    "    \"\"\"\n",
    "    Viterbi algorithm with robust probability handling\n",
    "    \"\"\"\n",
    "    n = len(sentence)\n",
    "    viterbi_matrix = defaultdict(dict)\n",
    "    backpointer = defaultdict(dict)\n",
    "    \n",
    "    # Small epsilon value to avoid log(0)\n",
    "    EPSILON = 1e-10\n",
    "    \n",
    "    # Initialize first step\n",
    "    for tag in all_tags:\n",
    "        # Handle emission probability\n",
    "        emission_prob = emission_params[tag].get(sentence[0], EPSILON)\n",
    "        \n",
    "        # Handle transition probability\n",
    "        trans_prob = transition_params['START'].get(tag, EPSILON)\n",
    "        \n",
    "        # Calculate log probabilities safely\n",
    "        if emission_prob <= 0:\n",
    "            emission_prob = EPSILON\n",
    "        if trans_prob <= 0:\n",
    "            trans_prob = EPSILON\n",
    "            \n",
    "        viterbi_matrix[0][tag] = math.log(trans_prob) + math.log(emission_prob)\n",
    "        backpointer[0][tag] = 'START'\n",
    "    \n",
    "    # Recursion\n",
    "    for t in range(1, n):\n",
    "        word = sentence[t]\n",
    "        for current_tag in all_tags:\n",
    "            max_prob = -float('inf')\n",
    "            best_prev_tag = None\n",
    "            \n",
    "            for prev_tag in all_tags:\n",
    "                # Get probabilities safely\n",
    "                trans_prob = transition_params[prev_tag].get(current_tag, EPSILON)\n",
    "                emission_prob = emission_params[current_tag].get(word, EPSILON)\n",
    "                \n",
    "                if trans_prob <= 0:\n",
    "                    trans_prob = EPSILON\n",
    "                if emission_prob <= 0:\n",
    "                    emission_prob = EPSILON\n",
    "                \n",
    "                current_prob = (viterbi_matrix[t-1][prev_tag] + \n",
    "                               math.log(trans_prob) + \n",
    "                               math.log(emission_prob))\n",
    "                \n",
    "                if current_prob > max_prob:\n",
    "                    max_prob = current_prob\n",
    "                    best_prev_tag = prev_tag\n",
    "            \n",
    "            viterbi_matrix[t][current_tag] = max_prob\n",
    "            backpointer[t][current_tag] = best_prev_tag\n",
    "    \n",
    "    # Termination\n",
    "    max_prob = -float('inf')\n",
    "    best_last_tag = None\n",
    "    for tag in all_tags:\n",
    "        stop_prob = viterbi_matrix[n-1][tag] + math.log(transition_params[tag].get('STOP', EPSILON))\n",
    "        if stop_prob > max_prob:\n",
    "            max_prob = stop_prob\n",
    "            best_last_tag = tag\n",
    "    \n",
    "    # Backtrace\n",
    "    tags = [best_last_tag]\n",
    "    for t in range(n-1, 0, -1):\n",
    "        tags.append(backpointer[t][tags[-1]])\n",
    "    tags.reverse()\n",
    "    \n",
    "    return tags\n",
    "\n",
    "def run_viterbi_on_dev_set(dev_in_path, output_path, transition_params, emission_params, all_tags):\n",
    "    \"\"\"\n",
    "    Runs Viterbi on development set and writes predictions\n",
    "    \"\"\"\n",
    "    with open(dev_in_path, 'r', encoding='utf-8') as fin, \\\n",
    "         open(output_path, 'w', encoding='utf-8') as fout:\n",
    "        \n",
    "        current_sentence = []\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                current_sentence.append(line)\n",
    "            else:\n",
    "                if current_sentence:\n",
    "                    predicted_tags = viterbi(current_sentence, transition_params, \n",
    "                                           emission_params, all_tags)\n",
    "                    for word, tag in zip(current_sentence, predicted_tags):\n",
    "                        fout.write(f\"{word} {tag}\\n\")\n",
    "                    fout.write(\"\\n\")\n",
    "                current_sentence = []\n",
    "        \n",
    "        # Handle last sentence if file doesn't end with newline\n",
    "        if current_sentence:\n",
    "            predicted_tags = viterbi(current_sentence, transition_params, \n",
    "                                   emission_params, all_tags)\n",
    "            for word, tag in zip(current_sentence, predicted_tags):\n",
    "                fout.write(f\"{word} {tag}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "emission_params = compute_emission_parameters_smoothing(train_file)\n",
    "transition_params = compute_transition_parameters(train_file)\n",
    "    \n",
    "\n",
    "all_tags = set()\n",
    "with open(train_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            try:\n",
    "                _, tag = line.split()\n",
    "                all_tags.add(tag)\n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "# Run Viterbi on dev set\n",
    "run_viterbi_on_dev_set(dev_in_file, \"EN/dev.p2.out\", transition_params, emission_params, all_tags)\n",
    "    \n",
    "# Evaluate\n",
    "# metrics = evaluate(gold_file, \"EN/dev.p2.out\")\n",
    "# print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: python\n"
     ]
    }
   ],
   "source": [
    "!python EvalScript/evalResult.py EN/dev.out EN/dev.p2.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 (25 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kth_best_viterbi(sentence, transition_params, emission_params, all_tags, k=4):\n",
    "    \"\"\"\n",
    "    Modified Viterbi algorithm to find the k-th best sequence\n",
    "    \"\"\"\n",
    "    n = len(sentence)\n",
    "    viterbi_matrix = defaultdict(dict)  # viterbi_matrix[t][tag] = list of top k probabilities\n",
    "    backpointer = defaultdict(dict)     # backpointer[t][tag] = list of top k previous tags\n",
    "    \n",
    "    EPSILON = 1e-10\n",
    "    \n",
    "    # Initialize first step\n",
    "    for tag in all_tags:\n",
    "        emission_prob = emission_params[tag].get(sentence[0], EPSILON)\n",
    "        trans_prob = transition_params['START'].get(tag, EPSILON)\n",
    "        \n",
    "        if emission_prob <= 0:\n",
    "            emission_prob = EPSILON\n",
    "        if trans_prob <= 0:\n",
    "            trans_prob = EPSILON\n",
    "            \n",
    "        prob = math.log(trans_prob) + math.log(emission_prob)\n",
    "        viterbi_matrix[0][tag] = [prob]\n",
    "        backpointer[0][tag] = [('START', 0)]  # (prev_tag, path_index)\n",
    "    \n",
    "    # Recursion\n",
    "    for t in range(1, n):\n",
    "        word = sentence[t]\n",
    "        for current_tag in all_tags:\n",
    "            all_paths = []\n",
    "            \n",
    "            for prev_tag in all_tags:\n",
    "                if prev_tag not in viterbi_matrix[t-1]:\n",
    "                    continue\n",
    "                    \n",
    "                trans_prob = transition_params[prev_tag].get(current_tag, EPSILON)\n",
    "                emission_prob = emission_params[current_tag].get(word, EPSILON)\n",
    "                \n",
    "                if trans_prob <= 0:\n",
    "                    trans_prob = EPSILON\n",
    "                if emission_prob <= 0:\n",
    "                    emission_prob = EPSILON\n",
    "                \n",
    "                # For each of the top k paths to prev_tag\n",
    "                for path_idx, prev_prob in enumerate(viterbi_matrix[t-1][prev_tag]):\n",
    "                    current_prob = prev_prob + math.log(trans_prob) + math.log(emission_prob)\n",
    "                    all_paths.append((current_prob, prev_tag, path_idx))\n",
    "            \n",
    "            # Sort all paths and keep top k\n",
    "            all_paths.sort(reverse=True, key=lambda x: x[0])\n",
    "            top_k_paths = all_paths[:k]\n",
    "            \n",
    "            if top_k_paths:\n",
    "                viterbi_matrix[t][current_tag] = [prob for prob, _, _ in top_k_paths]\n",
    "                backpointer[t][current_tag] = [(prev_tag, path_idx) for _, prev_tag, path_idx in top_k_paths]\n",
    "    \n",
    "    # Termination - find top k paths ending with STOP\n",
    "    final_paths = []\n",
    "    for tag in all_tags:\n",
    "        if tag not in viterbi_matrix[n-1]:\n",
    "            continue\n",
    "            \n",
    "        stop_prob = math.log(transition_params[tag].get('STOP', EPSILON))\n",
    "        for path_idx, prob in enumerate(viterbi_matrix[n-1][tag]):\n",
    "            final_prob = prob + stop_prob\n",
    "            final_paths.append((final_prob, tag, path_idx))\n",
    "    \n",
    "    final_paths.sort(reverse=True, key=lambda x: x[0])\n",
    "    \n",
    "    # If there are fewer than k paths, return the last one\n",
    "    if len(final_paths) < k:\n",
    "        k = len(final_paths)\n",
    "    \n",
    "    # Get the k-th best path (0-indexed, so k=3 for 4th best)\n",
    "    if k == 0:\n",
    "        return []\n",
    "    \n",
    "    selected_path = final_paths[k-1]\n",
    "    final_tag, path_idx = selected_path[1], selected_path[2]\n",
    "    \n",
    "    # Backtrace\n",
    "    tags = [final_tag]\n",
    "    for t in range(n-1, 0, -1):\n",
    "        prev_tag, prev_path_idx = backpointer[t][tags[-1]][path_idx]\n",
    "        tags.append(prev_tag)\n",
    "        path_idx = prev_path_idx\n",
    "    \n",
    "    tags.reverse()\n",
    "    return tags\n",
    "\n",
    "def run_kth_best_viterbi(dev_in_path, output_path, transition_params, emission_params, all_tags, k=4):\n",
    "    \"\"\"\n",
    "    Runs k-th best Viterbi on development set and writes predictions\n",
    "    \"\"\"\n",
    "    with open(dev_in_path, 'r', encoding='utf-8') as fin, \\\n",
    "         open(output_path, 'w', encoding='utf-8') as fout:\n",
    "        \n",
    "        current_sentence = []\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                current_sentence.append(line)\n",
    "            else:\n",
    "                if current_sentence:\n",
    "                    predicted_tags = kth_best_viterbi(current_sentence, transition_params, \n",
    "                                                    emission_params, all_tags, k)\n",
    "                    for word, tag in zip(current_sentence, predicted_tags):\n",
    "                        fout.write(f\"{word} {tag}\\n\")\n",
    "                    fout.write(\"\\n\")\n",
    "                current_sentence = []\n",
    "        \n",
    "        # Handle last sentence if file doesn't end with newline\n",
    "        if current_sentence:\n",
    "            predicted_tags = kth_best_viterbi(current_sentence, transition_params, \n",
    "                                            emission_params, all_tags, k)\n",
    "            for word, tag in zip(current_sentence, predicted_tags):\n",
    "                fout.write(f\"{word} {tag}\\n\")\n",
    "\n",
    "# change k for k best variation\n",
    "run_kth_best_viterbi(dev_in_file, \"EN/dev.p3.out\", transition_params, emission_params, all_tags, k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: python\n"
     ]
    }
   ],
   "source": [
    "!python EvalScript/evalResult.py EN/dev.out EN/dev.p3.out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: python\n"
     ]
    }
   ],
   "source": [
    "!python EvalScript/evalResult.py EN/dev.out EN/dev.p4.out\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
